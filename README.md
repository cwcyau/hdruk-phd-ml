# HDRUK-Turing PhD Programme in Health Data Science: Machine Learning 

This is a repo for the Machine Learning training series for the Year 1 Cohort of the HDRUK-Turing PhD Programme in Health Data Science.

The purpose of this course is to provide students with an overview of the cutting edge ideas in modern machine learning with a focus on probabilistic approaches based on Bayesian methodology. We hope to enable further exploration of these topics as relevant to student's long-term research plans.

*Professor Christopher Yau*
*Co-Director, HDRUK-Turing PhD Programme in Health Data Science*
*University of Manchester*

## Programme Contents

The course consists of eight parts which are summarised below. Please check each individual part for course pre-requisites and pre-reading.

**1. [Introduction to Bayesian Machine Learning](intro-to-bayes.md)**

*Lecturer: Christopher Yau*

The purpose of this lecture is to provide an overview of the key steps of classical Bayesian analysis. We will then consider how Bayesian principles are applied in the current era and the implications of convergences in Statistics and Computation that have seen the emergence of a new Bayesian paradigm.

Topics covered include:
  - Principles of Bayesian analysis
  - De Finetti's Theorem
  - Decision Theory
  - Computational Statistics
  
**2. [Modelling in Machine Learning](ml-modelling.md) **

*Lecturer: Christopher Yau*

In this lecture, we will consider the principles of modelling in a machine learning settings. We will consider a number of case studies and focus on the modelling choices that went into the design of the machine learning methods.

Topics covered include:
  - Design of probabilistic models
  - Choosing inference techniques
  - Model checking and validation
  
**3. [Approximate Inference](approximate-inference.md) **

*Lecturer: Kaspar Martens*

Approximate inference techniques are vital to the production of scalable algorithms that can operate on large high-dimensional data sets. This lecture will discuss two frequently used inference methods in Bayesian machine learning: variational inference and stochastic gradient descent with case studies.

Topics covered include:
  - Variational inference
  - Stochastic gradient descent

**4. [Differentiable Programming](differentiable-programming.md) **

*Lecturer: Dominic Danks*

Differentiable programming is a programming paradigm in which programs are created so that they can be differentiated, usually via automatic differentiation, allowing the use of gradient based optimisation of parameters in the program. In this lecture, we will examine Google JAX - a high-performance machine learning research library - which provides a framework for easy automatic differentiation of Python/NumPy-based functions.

Topics covered include:
  - Principles of differentiable programming
  - Google JAX
 
